# https://github.com/big-data-europe/docker-spark
version: '3'
services:
    # docker-compose -f .\docker-compose-submit.yml up
    # Para ejecutar con docker (Â¡NO admite rutas relativas en los volumenes!):
    #   docker run --tty --interactive --network=mynet -v C:\Users\joseluis\OneDrive\TFM\scala-spark-test\target\scala-spark-test-1.0-SNAPSHOT.jar:/app/application.jar -e "SPARK_APPLICATION_MAIN_CLASS=org.example.App" -e "ENABLE_INIT_DAEMON=false" bde2020/spark-submit:3.0.1-hadoop3.2 /bin/bash
    #   docker run --network=mynet -v C:\Users\joseluis\OneDrive\TFM\scala-spark-test\target\scala-spark-test-1.0-SNAPSHOT.jar:/app/application.jar -e "SPARK_APPLICATION_MAIN_CLASS=org.example.App" -e "ENABLE_INIT_DAEMON=false" bde2020/spark-submit:3.0.1-hadoop3.2 /submit.sh
    #
    # export SPARK_MASTER_URL=spark://${SPARK_MASTER_NAME}:${SPARK_MASTER_PORT}
    # export SPARK_HOME=/spark
    # /spark/bin/spark-submit --class org.example.SimpleApp --master ${SPARK_MASTER_URL} ${SPARK_SUBMIT_ARGS} /target/scala-spark-test-1.0-SNAPSHOT.jar
    sparksubmit:
        image: bde2020/spark-submit:3.0.1-hadoop3.2
        container_name: sparksubmit
        networks:
            - mynet
        ports:
            - 44040:4040
        volumes:
            - ../../target/scala-spark-test-1.0-SNAPSHOT.jar:/app/application.jar
        environment:
            TZ: Europe/Madrid
            SPARK_PUBLIC_DNS: sparksubmit
            SPARK_MASTER: spark://sparkmaster:7077
            SPARK_MASTER_NAME: sparkmaster

            SPARK_APPLICATION_MAIN_CLASS: org.example.SimpleApp
            SPARK_APPLICATION_ARGS: ""
            ENABLE_INIT_DAEMON: "false"
        env_file:
            - ./hadoop.env

networks:
    mynet:
        external:
            name: mynet
